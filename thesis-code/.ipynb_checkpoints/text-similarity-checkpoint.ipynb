{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from math import*\n",
    "import re\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "logging.basicConfig(format='%(levelname)s:%(message)s', level=logging.DEBUG)\n",
    "\n",
    "\"\"\"\n",
    "Loads the DataFrame with all the TMT articles. More info on this can be found in part 1 of the TMT\n",
    "recommender article series:\n",
    "www.themarketingtechnologist.co/building-a-recommendation-engine-for-geek-setting-up-the-prerequisites-13/\n",
    ":return: DataFrame with the title, content, tags and author of all TMT articles\n",
    "\"\"\"\n",
    "df = pd.read_csv('articles.csv', encoding='utf-8')         # Load articles in a DataFrame\n",
    "df = df[['title', 'content_text', 'tags']]  # Slice to remove redundant columns\n",
    "logging.debug(\"Number of articles: {0}\\n\".format(len(df)))\n",
    "\n",
    "def assign_single_tag(x):\n",
    "    x = x.lower().split(\",\")[0]\n",
    "    return x if x != \"\" else \"None\"\n",
    "# Clean up tags formatting\n",
    "df['tags'] = df['tags'].apply(lambda x: x.replace(\"[\", \"\").replace(\"]\", \"\"))\n",
    "# Assign first tag\n",
    "df['tags_first'] = df['tags'].apply(lambda x: assign_single_tag(x))\n",
    "\n",
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    Tokenizes sequences of text and stems the tokens.\n",
    "    :param text: String to tokenize\n",
    "    :return: List with stemmed tokens\n",
    "    \"\"\"\n",
    "    tokens = nltk.WhitespaceTokenizer().tokenize(text)\n",
    "    tokens = list(set(re.sub(\"[^a-zA-Z\\']\", \"\", token) for token in tokens))\n",
    "    tokens = [word for word in tokens if word not in stopwords.words('english')]\n",
    "    tokens = list(set(re.sub(\"[^a-zA-Z]\", \"\", token) for token in tokens))\n",
    "    stems = []\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    for token in tokens:\n",
    "        token = stemmer.stem(token)\n",
    "        if token != \"\":\n",
    "            stems.append(token)\n",
    "    return stems\n",
    "\n",
    "def get_vectorizer(ngram_range=(1, 3), min_df=2, max_df=1.0):\n",
    "    \"\"\"\n",
    "    Define a binary CountVectorizer (Feature Presence) using n-grams and min and max document frequency\n",
    "    :param ngram_range: n-grams are created for all numbers within this range\n",
    "    :param min_df: min document frequency of features\n",
    "    :param max_df: max document frequency of features\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    vectorizer = CountVectorizer(ngram_range=ngram_range,\n",
    "                                 tokenizer=tokenize,\n",
    "                                 min_df=min_df,\n",
    "                                 max_df=max_df,\n",
    "                                 binary=True,\n",
    "                                 stop_words='english')\n",
    "    return vectorizer\n",
    "\n",
    "def reduce_dimensionality(X, n_features):\n",
    "    \"\"\"\n",
    "    Apply PCA or SVD to reduce dimension to n_features.\n",
    "    :param X:\n",
    "    :param n_features:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # Initialize reduction method: PCA or SVD\n",
    "    # reducer = PCA(n_components=n_features)\n",
    "    reducer = TruncatedSVD(n_components=n_features)\n",
    "    # Fit and transform data to n_features-dimensional space\n",
    "    reducer.fit(X)\n",
    "    X = reducer.transform(X)\n",
    "    logging.debug(\"Reduced number of features to {0}\".format(n_features))\n",
    "    logging.debug(\"Percentage explained: %s\\n\" % reducer.explained_variance_ratio_.sum())\n",
    "    return X\n",
    "\n",
    "X_title = None\n",
    "n_features_title = 25\n",
    "vectorizer = get_vectorizer(ngram_range=(1, 2), min_df=2)\n",
    "X_title = vectorizer.fit_transform(df['title'])\n",
    "X_title = X_title.toarray()\n",
    "X_title = np.array(X_title, dtype=float)\n",
    "logging.debug(\"Number of features in title: {0}\".format(len(vectorizer.vocabulary_)))\n",
    "# Reduce dimensionality of title features\n",
    "X_title = reduce_dimensionality(X_title, n_features=n_features_title)\n",
    "print(X_title)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
