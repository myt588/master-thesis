{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bag_of_words with df as 3 and stop words: <2000x8988 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 160439 stored elements in Compressed Sparse Row format>\n",
      "First 30 features:\n",
      "['00', '000', '00pm', '01', '06', '08', '10', '100', '10th', '11', '12', '13', '13th', '14', '15', '150', '16', '17', '18', '180', '19', '1920', '1920s', '1928', '1930', '1930s', '1931', '1933', '1936', '1937']\n",
      "\n",
      "Features 1000 to 1030:\n",
      "['br', 'brad', 'braga', 'brain', 'brainer', 'brainless', 'brains', 'brand', 'bravado', 'brave', 'bravery', 'bravo', 'brazil', 'bread', 'break', 'breakdance', 'breakdancing', 'breakdown', 'breakers', 'breakfast', 'breakin', 'breaking', 'breaks', 'breakup', 'breasts', 'breath', 'breathe', 'breathing', 'breathtaking', 'breezy']\n",
      "\n",
      "Features 2000 to 2030:\n",
      "['currie', 'curse', 'curtain', 'curtis', 'cusack', 'custer', 'customers', 'customs', 'cut', 'cute', 'cuteness', 'cuter', 'cutouts', 'cuts', 'cutting', 'cybill', 'cycle', 'cynical', 'cynicism', 'da', 'dad', 'daddy', 'dagger', 'daily', 'daisenso', 'dale', 'dallas', 'dama', 'damage', 'damaged']\n",
      "\n",
      "tf-idf bag_of_words with df as 3 and stop words: <2000x8988 sparse matrix of type '<class 'numpy.float64'>'\n",
      "\twith 160439 stored elements in Compressed Sparse Row format>\n",
      "\n",
      "Features with lowest tfidf:\n",
      "\n",
      "['economic' 'miner' 'tumble' 'tantamount' 'fixed' 'newfound' 'bets'\n",
      " 'stomping' 'reconcile' 'ethnicities']\n",
      "\n",
      "Features with highest tfidf:\n",
      "\n",
      "['zombie' 'morty' 'cinderella' 'puppet' 'trauma' 'jill' 'timon' 'xica'\n",
      " 'zizek' 'br']\n",
      "\n",
      "Features with lowest idf:\n",
      "\n",
      "['movie' 'br' 'film' 'like' 'just' 'good' 'story' 'time' 'really' 'don']\n",
      "\n",
      "tf-idf bag_of_words with df as 3 and stop words: <2000x6010 sparse matrix of type '<class 'numpy.float64'>'\n",
      "\twith 34491 stored elements in Compressed Sparse Row format>\n",
      "\n",
      "2-gram : Features with lowest tfidf:\n",
      "\n",
      "['plot turns' 'star luke' 'little does' 'months later' 'episode ii'\n",
      " 'looks exactly' 'dripping blood' 'gore isn' 'character real' 'man just']\n",
      "\n",
      "2-gram : Features with highest tfidf:\n",
      "\n",
      "['chris sarandon' 'film beautiful' 'credits roll' 'definitely worst'\n",
      " 'low budget' 'movie pure' 'favorite scenes' 'br br' 'barbara bach'\n",
      " 'trust say']\n",
      "\n",
      "2-gram : Features with lowest idf:\n",
      "\n",
      "['br br' 've seen' 'br film' 'special effects' 'br movie' 'movie br'\n",
      " 'film br' 'good movie' 'br story' 'high school']\n",
      "\n",
      "The content of text in nth :\n",
      "\n",
      "[   4 1880   86 1821 1505]\n",
      "\n",
      "cosine similarities :\n",
      "\n",
      "[ 1.          0.4640074   0.4421208   0.32091389  0.30893006]\n",
      "\n",
      "topics, words: (20, 8988)\n",
      "\n",
      "topic 0       topic 1       topic 2       topic 3       topic 4       \n",
      "--------      --------      --------      --------      --------      \n",
      "movie         brosnan       movie         br            movie         \n",
      "br            film          film          film          great         \n",
      "like          kinnear       like          like          ramones       \n",
      "funny         time          just          story         school        \n",
      "comedy        br            good          just          high          \n",
      "matthau       pierce        br            good          br            \n",
      "burns         bond          really        movie         rock          \n",
      "just          greg          movies        films         love          \n",
      "best          xica          seen          horror        game          \n",
      "good          life          think         love          music         \n",
      "\n",
      "\n",
      "topic 5       topic 6       topic 7       topic 8       topic 9       \n",
      "--------      --------      --------      --------      --------      \n",
      "br            br            scarecrow     br            br            \n",
      "movie         star          kid           film          good          \n",
      "bad           luke          preston       man           just          \n",
      "just          wars          film          life          film          \n",
      "like          movie         movie         world         star          \n",
      "good          vader         just          story         trek          \n",
      "film          new           zuniga        father        series        \n",
      "really        jedi          br            like          like          \n",
      "don           battle        corn          just          movie         \n",
      "movies        good          people        miike         enterprise    \n",
      "\n",
      "\n",
      "topic 10      topic 11      topic 12      topic 13      topic 14      \n",
      "--------      --------      --------      --------      --------      \n",
      "movie         ned           br            movie         br            \n",
      "film          kelly         christmas     bad           movie         \n",
      "amrita        br            story         good          slasher       \n",
      "heaven        zizek         film          just          just          \n",
      "shahid        film          scrooge       watch         really        \n",
      "good          ledger        scott         movies        like          \n",
      "br            police        version       sandler       film          \n",
      "love          story         best          time          terrible      \n",
      "scene         heath         character     really        don           \n",
      "vivah         australian    george        make          make          \n",
      "\n",
      "\n",
      "topic 15      topic 16      topic 17      topic 18      topic 19      \n",
      "--------      --------      --------      --------      --------      \n",
      "film          like          stewart       film          br            \n",
      "great         alexandre     br            br            film          \n",
      "time          la            davies        carla         movie         \n",
      "br            movie         jeff          chess         story         \n",
      "price         hip           people        paul          like          \n",
      "red           eustache      gannon        characters    time          \n",
      "horror        hop           marion        luzhin        just          \n",
      "story         just          mann          office        good          \n",
      "zu            br            dawson        story         character     \n",
      "little        veronika      jimmy         love          man           \n",
      "\n",
      "\n",
      "topics, words: (10, 8988)\n",
      "\n",
      "topic 0       topic 1       topic 2       topic 3       topic 4       \n",
      "--------      --------      --------      --------      --------      \n",
      "br            film          movie         br            movie         \n",
      "movie         br            like          film          br            \n",
      "comedy        brosnan       br            like          great         \n",
      "funny         best          just          just          film          \n",
      "davies        spielberg     film          story         high          \n",
      "matthau       danny         good          good          school        \n",
      "like          time          really        movie         love          \n",
      "burns         kinnear       movies        films         good          \n",
      "film          life          bad           character     ramones       \n",
      "just          movie         time          plot          story         \n",
      "\n",
      "\n",
      "topic 5       topic 6       topic 7       topic 8       topic 9       \n",
      "--------      --------      --------      --------      --------      \n",
      "br            br            red           br            br            \n",
      "movie         star          giallo        film          stewart       \n",
      "just          film          evelyn        story         jeff          \n",
      "like          movie         queen         life          trek          \n",
      "film          luke          lady          man           man           \n",
      "good          wars          poirot        like          like          \n",
      "bad           death         italian       world         dawson        \n",
      "really        chess         times         time          film          \n",
      "time          story         kills         movie         just          \n",
      "story         love          miraglia      just          star          \n",
      "\n",
      "\n",
      "Vocabulary size: \n",
      "\n",
      "Vocabulary content:\n",
      "\n",
      "article :\n",
      "\n",
      "extracted nouns :\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 下記の各問に解答しなさい\n",
    "# インポート\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import mglearn\n",
    "from IPython.display import display\n",
    "from sklearn.datasets import load_files\n",
    "%matplotlib inline\n",
    "\n",
    "# 映画レビューのテキストファイル集合を読み込む\n",
    "reviews = load_files(\"/root/userspace/webeng20170607/data/IMDb/\")\n",
    "review_text, review_label = reviews.data, reviews.target\n",
    "\n",
    "#----------------------------------------------------------------\n",
    "# Q1 : 辞書構築し中身を確認する\n",
    "#----------------------------------------------------------------\n",
    "#  映画レビューデータセットの文書集合について、少なくとも3つ以上の文書に出現する単語を用いて辞書を構築する。\n",
    "#  辞書構築の際は、英語のストップワードリストを用いる。\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# 処理を記入\n",
    "\n",
    "review_vect = CountVectorizer(min_df=3, stop_words=\"english\").fit(review_text)\n",
    "review_bow = review_vect.transform(review_text)\n",
    "print(\"bag_of_words with df as 3 and stop words: {}\".format(repr(review_bow)))\n",
    "# Q1- Answer : 辞書の中身を確認 \n",
    "review_features = review_vect.get_feature_names()\n",
    "print(\"First 30 features:\\n{}\\n\".format(review_features[:30]))\n",
    "print(\"Features 1000 to 1030:\\n{}\\n\".format(review_features[1000:1030]))\n",
    "print(\"Features 2000 to 2030:\\n{}\\n\".format(review_features[2000:2030]))\n",
    "\n",
    "\n",
    "#----------------------------------------------------------------\n",
    "# Q2 : tfidfベクトルの作成 し -> tf-idf BoW表現を構築 \n",
    "#----------------------------------------------------------------\n",
    "#  文書の長さがベクトル表現に影響しないようにするため、tfidfをL2正則化により、それぞれ文書表現の長さが1になるようにする。\n",
    "#  正則化は、TfidfVectorizerの\"norm\"パラメータに指定します。L2正則化の場合、パラメータ値は\"l2\"となります。\n",
    "#  テキストのBoW表現を構築（SciPyの疎行列として格納） TfidfTransformerでCountVectorizerの疎行列を変換してもよい。\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# 処理を記入\n",
    "review_vect_tfidf =  TfidfVectorizer(min_df=3, stop_words=\"english\", norm=\"l2\").fit(review_text)\n",
    "review_bow_tfidf = review_vect_tfidf.transform(review_text)\n",
    "\n",
    "print(\"tf-idf bag_of_words with df as 3 and stop words: {}\\n\".format(repr(review_bow_tfidf)))\n",
    "# Q2 - Answer: テキストのtf-idf BoW表現を出力\n",
    "\n",
    "\n",
    "#----------------------------------------------------------------\n",
    "# Q3 : tfidfベクトルを用いて、tfidfの高い語、tfidfの低い語、idfの低い語 各10件確認する\n",
    "#----------------------------------------------------------------\n",
    "# 処理を記入\n",
    "\n",
    "max_value = review_bow_tfidf .max(axis=0).toarray().ravel()\n",
    "sorted_by_tfidf = max_value.argsort()\n",
    "\n",
    "feature_names = np.array(review_vect_tfidf.get_feature_names())\n",
    "# Q3- Answer1: tfidfの低い語を確認\n",
    "print(\"Features with lowest tfidf:\")\n",
    "print(\"\\n{}\\n\".format(feature_names[sorted_by_tfidf[:10]]))\n",
    "\n",
    "# Q3- Answer2: tfidfの高い語を確認\n",
    "print(\"Features with highest tfidf:\")\n",
    "print(\"\\n{}\\n\".format(feature_names[sorted_by_tfidf[-10:]]))\n",
    "\n",
    "# Q3- Answer3: idfの低い（dfの高い）語を確認\n",
    "print(\"Features with lowest idf:\")\n",
    "sorted_by_idf = np.argsort(review_vect_tfidf.idf_)\n",
    "print(\"\\n{}\\n\".format(feature_names[sorted_by_idf[:10]]))\n",
    "#----------------------------------------------------------------\n",
    "\n",
    "\n",
    "# Q4 バイグラム(2-gram)を用いて、上記と同じ条件で各文書のtfidfベクトルを、各10件確認する\n",
    "# ----------------------------------------------------------------\n",
    "# ユニグラムではなくバイグラム(2-gram)を用いて、上記と同じ条件で各文書のtfidfベクトルを作成する。\n",
    "\n",
    "# 処理を記入\n",
    "\n",
    "#ngram_rangeパラメータでトークン列の長さの最小と最大を指定。この場合はバイグラムになる。\n",
    "review_vect_tfidf =  TfidfVectorizer(min_df=3, stop_words=\"english\",ngram_range=(2,2), norm=\"l2\").fit(review_text)\n",
    "review_bow_tfidf = review_vect_tfidf.transform(review_text)\n",
    "\n",
    "print(\"tf-idf bag_of_words with df as 3 and stop words: {}\\n\".format(repr(review_bow_tfidf)))\n",
    "\n",
    "# tfidfの最大値を見つける\n",
    "max_value = review_bow_tfidf .max(axis=0).toarray().ravel()\n",
    "sorted_by_tfidf = max_value.argsort()\n",
    "\n",
    "feature_names = np.array(review_vect_tfidf.get_feature_names())\n",
    "\n",
    "# Q4- Answer1: tfidfの低い語を確認\n",
    "print(\"2-gram : Features with lowest tfidf:\")\n",
    "print(\"\\n{}\\n\".format(feature_names[sorted_by_tfidf[:10]]))\n",
    "\n",
    "# Q4- Answer2: tfidfの高い語を確認\n",
    "print(\"2-gram : Features with highest tfidf:\")\n",
    "print(\"\\n{}\\n\".format(feature_names[sorted_by_tfidf[-10:]]))\n",
    "\n",
    "# Q4- Answer3: idfの低い（dfの高い）語を確認\n",
    "print(\"2-gram : Features with lowest idf:\")\n",
    "sorted_by_idf = np.argsort(review_vect_tfidf.idf_)\n",
    "print(\"\\n{}\\n\".format(feature_names[sorted_by_idf[:10]]))\n",
    "\n",
    "\n",
    "\n",
    "# Q5: cosine類似度に基づいて、5番目のレビューに最も類似した上位5文書の内容と類似度を表示するプログラムを作成\n",
    "# ----------------------------------------------------------------\n",
    "# 上記の条件で作成したユニグラムのtfidfベクトルを用いる。\n",
    "# cosine類似度に基づいて任意の文書に最も類似した上位5文書の内容と類似度を表示するプログラムを作成する。\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# 処理を記入\n",
    "vect_tfidf =  TfidfVectorizer(min_df=3, stop_words=\"english\").fit(review_text)\n",
    "bow_tfidf = vect_tfidf.transform(review_text)\n",
    "\n",
    "cos_sim = cosine_similarity(bow_tfidf[4:5], bow_tfidf).flatten()\n",
    "\n",
    "# Q5 - Answer: ５番目のレビュー と 最も類似しているtop5のレビューの 文章と類似度を出力\n",
    "# for 文で出力\n",
    "top5_sim_docs = cos_sim.argsort()[:-6:-1]\n",
    "print(\"The content of text in nth :\")\n",
    "print(\"\\n{}\\n\".format(top5_sim_docs))\n",
    "print(\"cosine similarities :\")\n",
    "print(\"\\n{}\\n\".format(cos_sim[top5_sim_docs]))\n",
    "\n",
    "\n",
    "# Q6: トピックモデルのトピック数を変えて実行し、抽出される違いを確認する。 \n",
    "# ----------------------------------------------------------------\n",
    "# トピック数 20個の場合と 10個の場合で比較\n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# 処理を記入\n",
    "vect = CountVectorizer(min_df=3, stop_words=\"english\").fit(review_text)\n",
    "bow = vect.transform(review_text)\n",
    "\n",
    "\n",
    "# Q6 -Answer1 : トピック数を20としたLDAの実行\n",
    "lda = LatentDirichletAllocation(n_topics=20, learning_method=\"batch\",max_iter=25, random_state=0)\n",
    "doc_topics = lda.fit_transform(bow)\n",
    "print(\"topics, words: {}\\n\".format(lda.components_.shape))\n",
    "\n",
    "sorting = np.argsort(lda.components_, axis=1)[:, ::-1]\n",
    "#辞書語を取得\n",
    "feature_names = np.array(vect.get_feature_names())\n",
    "mglearn.tools.print_topics(topics=range(20), feature_names=feature_names,\n",
    "                           sorting=sorting, topics_per_chunk=5, n_words=10)\n",
    "\n",
    "# Q6 -Answer2 : トピック数を10としたLDAの実行\n",
    "lda = LatentDirichletAllocation(n_topics=10, learning_method=\"batch\",max_iter=25, random_state=0)\n",
    "doc_topics = lda.fit_transform(bow)\n",
    "print(\"topics, words: {}\\n\".format(lda.components_.shape))\n",
    "sorting = np.argsort(lda.components_, axis=1)[:, ::-1]\n",
    "#辞書語を取得\n",
    "feature_names = np.array(vect.get_feature_names())\n",
    "mglearn.tools.print_topics(topics=range(10), feature_names=feature_names,\n",
    "                           sorting=sorting, topics_per_chunk=5, n_words=10)\n",
    "\n",
    "\n",
    "# Q7.  日本語形態素解析を行う\n",
    "# ----------------------------------------------------------------\n",
    "# Q7-1 : 入力文データを変えてMecabを実行し、形態素解析の結果を確認する\n",
    "# Q7-2 : スクレピングで取得したyahoo Newsの記事から、名詞を抽出して出力するプログラムを作成する\n",
    "\n",
    "import MeCab\n",
    "\n",
    "#MeCabによる形態素解析\n",
    "class jp_pos_tagger:\n",
    "\n",
    "    def __init__(self, dictionary=\"mecabrc\"):\n",
    "        self.dictionary = dictionary\n",
    "        self.tagger = MeCab.Tagger(self.dictionary)\n",
    "    \n",
    "    def mecab_tagger_noun(self, text):\n",
    "        if not text:\n",
    "            return []\n",
    "        words = []\n",
    "        node = self.tagger.parseToNode(text)\n",
    "        while node:\n",
    "            features = node.feature.split(',')\n",
    "            #名詞のみ抽出\n",
    "            if features[0] in [\"名詞\"]:\n",
    "                if features[6] == \"*\":\n",
    "                    words.append(node.surface)\n",
    "                else:                    \n",
    "                    words.append(features[6])\n",
    "            node = node.next\n",
    "        return words\n",
    "    \n",
    "# Q7- Answer1 :入力文データを変えてMecabを実行し、形態素解析の結果を確認する\n",
    "# 処理を記入\n",
    "\n",
    "jp_text_data = [\n",
    "    '人工知能とは、コンピュータを使って、学習・推論・判断など人間の知能のはたらきを人工的に実現したもの',\n",
    "    'ウェブとは、インターネット上で提供されるハイパーテキストシステム',\n",
    "    '機械学習とは、人間が自然に行っている学習能力と同様の機能をコンピュータで実現しようとする技術・手法のこと'\n",
    "]\n",
    "pos = jp_pos_tagger()\n",
    "pos_vect = CountVectorizer(analyzer=pos.mecab_tagger)\n",
    "pos_vect.fit(jp_text_data)\n",
    "print(\"Vocabulary size: {}\".format(len(pos_vect.vocabulary_)))\n",
    "print(\"Vocabulary content:\\n {}\\n\".format(pos_vect.vocabulary_)) \n",
    "    \n",
    "# Q7- Answer2 : スクレピングで取得したyahoo Newsの記事から、名詞を抽出して出力するプログラムを作成する\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request as req\n",
    "import re\n",
    "\n",
    "url = \"https://news.yahoo.co.jp/list\"\n",
    "# 処理を記入\n",
    "res = req.urlopen(url)\n",
    "soup = BeautifulSoup(res, \"html.parser\")\n",
    "topics = soup.find_all(href=re.compile(\"^https://news.yahoo.co.jp/pickup\"))\n",
    "for topic in topics[:5]:\n",
    "    res = req.urlopen(topic.attrs['href'])\n",
    "    soup = BeautifulSoup(res, \"html.parser\")\n",
    "    news = soup.select_one(\"p.hbody\")\n",
    "    print(news)\n",
    "    print(\"\\n\")\n",
    "\n",
    "\n",
    "print(\"article :\")\n",
    "print()\n",
    "print(\"extracted nouns :\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 24\n",
      "Vocabulary content:\n",
      " {'学習': 12, '提供': 17, '判断': 10, 'インターネット': 3, 'ハイパーテキストシステム': 6, '能力': 22, '推論': 16, '人間': 9, 'コンピュータ': 5, 'ウェブ': 4, '機械': 18, 'はたらき': 1, '実現': 13, '上': 7, '知能': 21, '技術': 15, '機能': 19, '手法': 14, '的': 20, '人工': 8, '同様': 11, 'こと': 0, '自然': 23, 'もの': 2}\n",
      "\n",
      "<p class=\"hbody\">　サッカー・ロシアＷ杯アジア最終予選Ｂ組（１３日、イラク１－１日本、テヘラン）Ｂ組首位の日本は５位のイラクと対戦。ＦＷ大迫勇也（２７）＝ケルン＝のゴールで先制も、後半に同点とされドロー。けが人が続出する厳しい戦いとなったが敵地で貴重な勝ち点「１」を獲得。同組首位をキープし、Ｗ杯出場に王手をかけた。(サンケイスポーツ)</p>\n",
      "\n",
      "\n",
      "<p class=\"hbody\"></p>\n",
      "\n",
      "\n",
      "<p class=\"hbody\">　【北京時事】中国が2018年に打ち上げを予定している月面無人探査機「嫦娥4号」で、蚕やジャガイモなどの生育実験を計画している。(時事通信)</p>\n",
      "\n",
      "\n",
      "<p class=\"hbody\">　環境省は１３日、特定外来生物で強い毒を持つ南米原産の「ヒアリ」が、中国から神戸港に入港し尼崎市内に運ばれたコンテナの中で発見された、と発表した。国内での確認は初めて。(神戸新聞NEXT)</p>\n",
      "\n",
      "\n",
      "<p class=\"hbody\">　「検査」と称して女性の身体を触ったとして、強制わいせつ容疑などで埼玉県警に再逮捕された男が、成人向け漫画同人誌を読んで手口を真似したという趣旨の供述をしていることが13日、捜査関係者への取材で分かった。県警は被害の再発防止に向けて、漫画の作者に模倣した犯罪が起こらないよう配慮してほしいと要請した。県警によると、犯罪に模倣されたとして著作物の作者に申し入れをするのは異例。(埼玉新聞)</p>\n",
      "\n",
      "\n",
      "article :\n",
      "\n",
      "['\\u3000サッカー・ロシアＷ杯アジア最終予選Ｂ組（１３日、イラク１－１日本、テヘラン）Ｂ組首位の日本は５位のイラクと対戦。ＦＷ大迫勇也（２７）＝ケルン＝のゴールで先制も、後半に同点とされドロー。けが人が続出する厳しい戦いとなったが敵地で貴重な勝ち点「１」を獲得。同組首位をキープし、Ｗ杯出場に王手をかけた。(サンケイスポーツ)', None, '\\u3000【北京時事】中国が2018年に打ち上げを予定している月面無人探査機「嫦娥4号」で、蚕やジャガイモなどの生育実験を計画している。(時事通信)', '\\u3000環境省は１３日、特定外来生物で強い毒を持つ南米原産の「ヒアリ」が、中国から神戸港に入港し尼崎市内に運ばれたコンテナの中で発見された、と発表した。国内での確認は初めて。(神戸新聞NEXT)', '\\u3000「検査」と称して女性の身体を触ったとして、強制わいせつ容疑などで埼玉県警に再逮捕された男が、成人向け漫画同人誌を読んで手口を真似したという趣旨の供述をしていることが13日、捜査関係者への取材で分かった。県警は被害の再発防止に向けて、漫画の作者に模倣した犯罪が起こらないよう配慮してほしいと要請した。県警によると、犯罪に模倣されたとして著作物の作者に申し入れをするのは異例。(埼玉新聞)']\n",
      "\n",
      "extracted nouns :\n",
      "\n",
      "{'NEXT': 5, '中': 26, '手口': 65, '犯罪': 86, '県警': 97, '後半': 62, '年': 60, 'a': 6, '発見': 96, '月面': 75, 'Ｗ杯': 125, '時事': 72, '実験': 55, '環境省': 89, '生育': 91, '王手': 88, '入港': 35, '異例': 94, '神戸': 100, 'ロー': 25, '(': 0, '埼玉': 50, '発表': 95, '新聞': 69, '4': 4, '点': 82, '続出': 102, '日': 70, '大迫': 52, '１': 119, '尼崎': 58, '地': 49, '原産': 42, '13': 2, '国内': 48, '著作': 104, '2018': 3, 'サッカー': 18, '身体': 112, '毒': 79, '無人': 83, '防止': 116, '勝ち': 39, '者': 103, '５': 122, 'イラク': 13, '要請': 107, '被害': 106, '検査': 76, '位': 31, '作者': 32, '勇': 38, ')': 1, '同点': 46, 'アジア': 12, '模倣': 77, '機': 78, '強制': 61, '予選': 30, '男': 93, '３': 121, 'キープ': 14, '探査': 67, '漫画': 81, '蚕': 105, '最終': 74, 'こと': 8, '－': 118, '獲得': 87, 'ロシア': 24, '同人': 45, '生物': 90, '成人': 63, '計画': 108, '配慮': 114, '港': 80, 'よう': 10, 'スポーツ': 21, 'ジャガイモ': 20, 'コンテナ': 16, '関係': 115, 'ゴール': 17, '７': 123, 'サンケイ': 19, '先制': 34, '逮捕': 113, 'テヘラン': 22, 'ＦＷ': 124, '時事通信': 73, '誌': 109, '捜査': 66, '物': 84, '特定': 85, '確認': 99, 'ヒアリ': 23, 'わいせつ': 11, '北京': 40, '嫦娥': 54, 'ケルン': 15, '中国': 27, '予定': 29, '２': 120, '対戦': 57, '也': 28, '出場': 37, 'けが人': 7, '容疑': 56, '組': 101, '敵': 68, '首位': 117, '日本': 71, '真似': 98, '号': 44, '戦い': 64, '向け': 47, '申し入れ': 92, '貴重': 110, '南米': 41, '外来': 51, 'の': 9, '女性': 53, '趣旨': 111, '市内': 59, '再発': 36, '供述': 33, '取材': 43}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Q7.  日本語形態素解析を行う\n",
    "# ----------------------------------------------------------------\n",
    "# Q7-1 : 入力文データを変えてMecabを実行し、形態素解析の結果を確認する\n",
    "# Q7-2 : スクレピングで取得したyahoo Newsの記事から、名詞を抽出して出力するプログラムを作成する\n",
    "\n",
    "import MeCab\n",
    "\n",
    "#MeCabによる形態素解析\n",
    "class jp_pos_tagger:\n",
    "\n",
    "    def __init__(self, dictionary=\"mecabrc\"):\n",
    "        self.dictionary = dictionary\n",
    "        self.tagger = MeCab.Tagger(self.dictionary)\n",
    "    \n",
    "    def mecab_tagger_noun(self, text):\n",
    "        if not text:\n",
    "            return []\n",
    "        words = []\n",
    "        node = self.tagger.parseToNode(text)\n",
    "        while node:\n",
    "            features = node.feature.split(',')\n",
    "            #名詞のみ抽出\n",
    "            if features[0] in [\"名詞\"]:\n",
    "                if features[6] == \"*\":\n",
    "                    words.append(node.surface)\n",
    "                else:                    \n",
    "                    words.append(features[6])\n",
    "            node = node.next\n",
    "        return words\n",
    "    \n",
    "# Q7- Answer1 :入力文データを変えてMecabを実行し、形態素解析の結果を確認する\n",
    "# 処理を記入\n",
    "\n",
    "jp_text_data = [\n",
    "    '人工知能とは、コンピュータを使って、学習・推論・判断など人間の知能のはたらきを人工的に実現したもの',\n",
    "    'ウェブとは、インターネット上で提供されるハイパーテキストシステム',\n",
    "    '機械学習とは、人間が自然に行っている学習能力と同様の機能をコンピュータで実現しようとする技術・手法のこと'\n",
    "]\n",
    "pos = jp_pos_tagger()\n",
    "pos_vect = CountVectorizer(analyzer=pos.mecab_tagger_noun)\n",
    "pos_vect.fit(jp_text_data)\n",
    "print(\"Vocabulary size: {}\".format(len(pos_vect.vocabulary_)))\n",
    "print(\"Vocabulary content:\\n {}\\n\".format(pos_vect.vocabulary_)) \n",
    "    \n",
    "# Q7- Answer2 : スクレピングで取得したyahoo Newsの記事から、名詞を抽出して出力するプログラムを作成する\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request as req\n",
    "import re\n",
    "\n",
    "url = \"https://news.yahoo.co.jp/list\"\n",
    "# 処理を記入\n",
    "res = req.urlopen(url)\n",
    "soup = BeautifulSoup(res, \"html.parser\")\n",
    "topics = soup.find_all(href=re.compile(\"^https://news.yahoo.co.jp/pickup\"))\n",
    "texts = []\n",
    "for topic in topics[:5]:\n",
    "    res = req.urlopen(topic.attrs['href'])\n",
    "    soup = BeautifulSoup(res, \"html.parser\")\n",
    "    news = soup.select_one(\"p.hbody\")\n",
    "    texts.append(news.string)\n",
    "    print(news)\n",
    "    print(\"\\n\")\n",
    "\n",
    "pos = jp_pos_tagger()\n",
    "pos_vect = CountVectorizer(analyzer=pos.mecab_tagger_noun)\n",
    "pos_vect.fit(texts)\n",
    "print(\"article :\")\n",
    "print(\"\\n{}\\n\".format(texts))\n",
    "print(\"extracted nouns :\")\n",
    "print(\"\\n{}\\n\".format(pos_vect.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
